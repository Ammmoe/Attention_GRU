{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f01ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from data.trajectory_loader import load_dataset\n",
    "from models.attention_bi_lstm_predictor import TrajPredictor\n",
    "from utils.logger import get_logger\n",
    "from utils.model_evaluator import evaluate_metrics_multi_agent_per_timestep as evaluate\n",
    "from utils.plot_generator import plot_trajectories, plot_3d_trajectories_subplots\n",
    "from utils.scaler import scale_per_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044612ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pylint: disable=all\n",
    "# Data settings and parameters\n",
    "DATA_TYPE = \"zurich\"  # Options: \"zurich\", \"quadcopter\", \"mixed\"\n",
    "SEQUENTIAL_PREDICTION = (\n",
    "    True  # If False, model predicts only the last point for FORWARD_LEN steps\n",
    ")\n",
    "AGENTS = 3  # Number of agents or drones\n",
    "LOOK_BACK = 50  # Number of past time steps to use as input\n",
    "FORWARD_LEN = 5  # Number of future time steps to predict\n",
    "FEATURES_PER_AGENT = 3  # x, y, z\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Plotting parameters\n",
    "NUM_PLOTS = 2  # number of plots to generate\n",
    "NUM_SUBPLOTS = 2  # number of subplots to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc6e2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logger and experiment folder\n",
    "logger, exp_dir = get_logger()\n",
    "os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "logger.info(\"Experiment started\")\n",
    "logger.info(\"Experiment folder: %s\", exp_dir)\n",
    "logger.info(\"Dataset: %s\", DATA_TYPE)\n",
    "logger.info(\"Number of drones (agents): %d\", AGENTS)\n",
    "logger.info(\"LOOK_BACK (past steps): %d\", LOOK_BACK)\n",
    "logger.info(\"FORWARD_LEN (future steps): %d\", FORWARD_LEN)\n",
    "logger.info(\"SEQUENTIAL_PREDICTION: %s\", \"True\" if SEQUENTIAL_PREDICTION else \"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9a92da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrame\n",
    "df = load_dataset(\n",
    "    DATA_TYPE,\n",
    "    min_rows=800,\n",
    "    num_flights=AGENTS,\n",
    ")\n",
    "print(df.shape)\n",
    "\n",
    "# Prepare sequences\n",
    "# Track trajectory indices to be used in plotting later\n",
    "X, y, trajectory_ids = [], [], []\n",
    "\n",
    "for traj_idx in df[\"trajectory_index\"].unique():\n",
    "    traj_df = df[df[\"trajectory_index\"] == traj_idx].reset_index(drop=True)\n",
    "\n",
    "    # Drop trajectory_index for features\n",
    "    traj_data = traj_df.drop(columns=[\"trajectory_index\"]).values.astype(np.float32)\n",
    "    n_rows = traj_data.shape[0]\n",
    "\n",
    "    seq_count = n_rows - LOOK_BACK - FORWARD_LEN + 1\n",
    "    for i in range(seq_count):\n",
    "        seq_X = traj_data[i : i + LOOK_BACK]  # shape (LOOK_BACK, features)\n",
    "\n",
    "        # For sequential prediction, predict FORWARD_LEN steps; else just the last step\n",
    "        if SEQUENTIAL_PREDICTION:\n",
    "            seq_y = traj_data[\n",
    "                i + LOOK_BACK : i + LOOK_BACK + FORWARD_LEN\n",
    "            ]  # shape (FORWARD_LEN, features)\n",
    "        else:\n",
    "            seq_y = traj_data[\n",
    "                i + LOOK_BACK + FORWARD_LEN - 1 : i + LOOK_BACK + FORWARD_LEN\n",
    "            ]  # (1, features)\n",
    "\n",
    "        X.append(seq_X)\n",
    "        y.append(seq_y)\n",
    "        trajectory_ids.append(traj_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bd8e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy arrays\n",
    "X = np.array(X, dtype=np.float32)  # (num_sequences, LOOK_BACK, features)\n",
    "y = np.array(\n",
    "    y, dtype=np.float32\n",
    ")  # (num_sequences, 1, features) or (num_sequences, FORWARD_LEN, features)\n",
    "trajectory_ids = np.array(trajectory_ids)\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test, traj_train, traj_test = train_test_split(\n",
    "    X, y, trajectory_ids, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Get feature dimension dynamically\n",
    "num_features_X = X_train.shape[-1]\n",
    "\n",
    "# Scale data to [0, 1]\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit scalers on training data only\n",
    "X_train_scaled = scale_per_agent(X_train, scaler_X, FEATURES_PER_AGENT, fit=True)\n",
    "X_test_scaled = scale_per_agent(X_test, scaler_X, FEATURES_PER_AGENT, fit=False)\n",
    "\n",
    "y_train_scaled = scale_per_agent(y_train, scaler_y, FEATURES_PER_AGENT, fit=True)\n",
    "y_test_scaled = scale_per_agent(y_test, scaler_y, FEATURES_PER_AGENT, fit=False)\n",
    "\n",
    "# Save scalers\n",
    "joblib.dump(scaler_X, os.path.join(exp_dir, \"scaler_X.pkl\"))\n",
    "joblib.dump(scaler_y, os.path.join(exp_dir, \"scaler_y.pkl\"))\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(\n",
    "    X_train_scaled, dtype=torch.float32\n",
    ")  # (num_sequences, LOOK_BACK, features)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(X_train_tensor, y_train_tensor), batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    TensorDataset(X_test_tensor, y_test_tensor), batch_size=BATCH_SIZE, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922def7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log dataset sizes\n",
    "total_sequences = X_train_tensor.shape[0] + X_test_tensor.shape[0]\n",
    "logger.info(\"Total sequences: %d\", total_sequences)\n",
    "logger.info(\"Train sequences: %s\", X_train_tensor.shape)\n",
    "logger.info(\"Test sequences: %s\", X_test_tensor.shape)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(\"Using device: %s\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a11b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model, criterion, optimizer\n",
    "model_params = {\n",
    "    \"enc_hidden_size\": 64,\n",
    "    \"dec_hidden_size\": 64,\n",
    "    \"num_layers\": 1,\n",
    "}\n",
    "model = TrajPredictor(**model_params).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Log model info\n",
    "logger.info(\"Model module: %s\", model.__class__.__module__)\n",
    "logger.info(\"Model architecture:\\n%s\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c35e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log training time\n",
    "training_start = time.time()\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 10\n",
    "best_loss = float(\"inf\")\n",
    "epochs_no_improve = 0\n",
    "early_stop = False\n",
    "\n",
    "# Training Loop\n",
    "model.train()\n",
    "try:\n",
    "    for epoch in range(EPOCHS):\n",
    "        total_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Add prediction_len argument if sequential prediction\n",
    "            if SEQUENTIAL_PREDICTION:\n",
    "                pred = model(batch_x, pred_len=FORWARD_LEN)\n",
    "            else:\n",
    "                pred = model(batch_x, pred_len=1)\n",
    "\n",
    "            loss = criterion(pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        logger.info(\"Epoch %d/%d - Train Loss: %.7f\", epoch + 1, EPOCHS, avg_loss)\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), os.path.join(exp_dir, \"best_model.pt\"))\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # If no improvement for 'patience' epochs, stop training\n",
    "        if epochs_no_improve >= patience:\n",
    "            logger.info(\"Early stopping triggered after %d epochs\", epoch + 1)\n",
    "            early_stop = True\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    logger.warning(\"Training interrupted by user! Running evaluation...\")\n",
    "\n",
    "# Save last-epoch model\n",
    "finally:\n",
    "    torch.save(model.state_dict(), os.path.join(exp_dir, \"last_model.pt\"))\n",
    "\n",
    "# If training completed without early stopping\n",
    "if not early_stop:\n",
    "    logger.info(\"Training finished without early stopping.\")\n",
    "\n",
    "# Log total training time\n",
    "training_end_time = time.time()\n",
    "elapsed_time = training_end_time - training_start\n",
    "logger.info(\"Total training time: %.2f seconds\", elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b7de84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation parameters\n",
    "all_preds, all_trues = [], []\n",
    "inference_times = []\n",
    "test_loss = 0.0\n",
    "total_sequences = 0\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in test_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        total_sequences += batch_x.size(0)\n",
    "\n",
    "        # Measure inference time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Add prediction_len argument if sequential prediction\n",
    "        if SEQUENTIAL_PREDICTION:\n",
    "            outputs = model(batch_x, pred_len=FORWARD_LEN)\n",
    "        else:\n",
    "            outputs = model(batch_x, pred_len=1)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Record inference time per batch\n",
    "        inference_times.append(end_time - start_time)\n",
    "\n",
    "        # Compute test loss\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Store predictions and true values\n",
    "        all_preds.append(outputs.cpu())\n",
    "        all_trues.append(batch_y.cpu())\n",
    "\n",
    "# Compute average test loss\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "# Compute total inference time, average inference time per batch and per sequence\n",
    "total_inf_time = sum(inference_times)\n",
    "avg_inf_time_per_seq = total_inf_time / total_sequences\n",
    "avg_inf_time_per_batch = total_inf_time / len(test_loader)\n",
    "\n",
    "# Log final test metrics\n",
    "logger.info(\"Test Loss (scaled): %.7f\", avg_test_loss)\n",
    "logger.info(\"Average inference time per sequence: %.6f seconds\", avg_inf_time_per_seq)\n",
    "logger.info(\"Average inference time per batch: %.6f seconds\", avg_inf_time_per_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05586c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all batches\n",
    "y_pred = torch.cat(all_preds, dim=0)\n",
    "y_true = torch.cat(all_trues, dim=0)\n",
    "\n",
    "# Compute evaluation metrics (inverse scaling applied)\n",
    "mse_t, rmse_t, mae_t, ede_t, axis_mse_t, axis_rmse_t, axis_mae_t = evaluate(\n",
    "    y_true, y_pred, scaler_y, num_agents=AGENTS\n",
    ")\n",
    "\n",
    "# Table header\n",
    "header = (\n",
    "    f\"{'Timestep':>8} | {'EDE':>10} | {'MSE':>10} | {'RMSE':>10} | {'MAE':>10} | \"\n",
    "    f\"{'MSE_x':>10} {'MSE_y':>10} {'MSE_z':>10} | \"\n",
    "    f\"{'RMSE_x':>10} {'RMSE_y':>10} {'RMSE_z':>10} | \"\n",
    "    f\"{'MAE_x':>10} {'MAE_y':>10} {'MAE_z':>10}\"\n",
    ")\n",
    "logger.info(\"-\" * len(header))\n",
    "logger.info(header)\n",
    "logger.info(\"-\" * len(header))\n",
    "\n",
    "# Table rows\n",
    "for t, (ede, mse, rmse, mae, axis_mse, axis_rmse, axis_mae) in enumerate(\n",
    "    zip(ede_t, mse_t, rmse_t, mae_t, axis_mse_t, axis_rmse_t, axis_mae_t)\n",
    "):\n",
    "    logger.info(\n",
    "        \"%8d | %10.6f | %10.6f | %10.6f | %10.6f | \"\n",
    "        \"%10.6f %10.6f %10.6f | \"\n",
    "        \"%10.6f %10.6f %10.6f | \"\n",
    "        \"%10.6f %10.6f %10.6f\",\n",
    "        t,\n",
    "        ede,\n",
    "        mse,\n",
    "        rmse,\n",
    "        mae,\n",
    "        axis_mse[0],\n",
    "        axis_mse[1],\n",
    "        axis_mse[2],\n",
    "        axis_rmse[0],\n",
    "        axis_rmse[1],\n",
    "        axis_rmse[2],\n",
    "        axis_mae[0],\n",
    "        axis_mae[1],\n",
    "        axis_mae[2],\n",
    "    )\n",
    "\n",
    "# Summary averages\n",
    "logger.info(\"-\" * len(header))\n",
    "logger.info(\n",
    "    \"%8s | %10.6f | %10.6f | %10.6f | %10.6f | \"\n",
    "    \"%10.6f %10.6f %10.6f | \"\n",
    "    \"%10.6f %10.6f %10.6f | \"\n",
    "    \"%10.6f %10.6f %10.6f\",\n",
    "    \"Average\",\n",
    "    ede_t.mean(),\n",
    "    mse_t.mean(),\n",
    "    rmse_t.mean(),\n",
    "    mae_t.mean(),\n",
    "    axis_mse_t.mean(axis=0)[0],\n",
    "    axis_mse_t.mean(axis=0)[1],\n",
    "    axis_mse_t.mean(axis=0)[2],\n",
    "    axis_rmse_t.mean(axis=0)[0],\n",
    "    axis_rmse_t.mean(axis=0)[1],\n",
    "    axis_rmse_t.mean(axis=0)[2],\n",
    "    axis_mae_t.mean(axis=0)[0],\n",
    "    axis_mae_t.mean(axis=0)[1],\n",
    "    axis_mae_t.mean(axis=0)[2],\n",
    ")\n",
    "logger.info(\"-\" * len(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661fe40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save config / hyperparameters\n",
    "config = {\n",
    "    \"device\": str(device),\n",
    "    \"model_module\": model.__class__.__module__,\n",
    "    \"model_class\": model.__class__.__name__,\n",
    "    \"model_params\": model_params,\n",
    "    \"DATA_TYPE\": DATA_TYPE,\n",
    "    \"AGENTS\": AGENTS,\n",
    "    \"FEATURES_PER_AGENT\": FEATURES_PER_AGENT,\n",
    "    \"LOOK_BACK\": LOOK_BACK,\n",
    "    \"FORWARD_LEN\": FORWARD_LEN,\n",
    "    \"SEQUENTIAL_PREDICTION\": SEQUENTIAL_PREDICTION,\n",
    "    \"EPOCHS\": EPOCHS,\n",
    "    \"BATCH_SIZE\": BATCH_SIZE,\n",
    "    \"LEARNING_RATE\": LEARNING_RATE,\n",
    "}\n",
    "\n",
    "config_path = os.path.join(exp_dir, \"config.json\")\n",
    "with open(config_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "logger.info(\"Config saved to %s\", config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f1e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group results back by trajectory_index\n",
    "traj_test = traj_test[: len(y_true)]  # align just in case\n",
    "unique_trajs = np.unique(traj_test)\n",
    "\n",
    "# Randomly select trajectories to plot\n",
    "plot_trajs = np.random.choice(\n",
    "    unique_trajs, size=min(NUM_PLOTS, len(unique_trajs)), replace=False\n",
    ").tolist()\n",
    "\n",
    "# Plot trajectories using the helper function\n",
    "plot_trajectories(\n",
    "    y_true=y_true.numpy(),\n",
    "    y_pred=y_pred.numpy(),\n",
    "    traj_ids=traj_test,\n",
    "    plot_trajs=plot_trajs,\n",
    "    scaler=scaler_y,\n",
    "    agents=AGENTS,\n",
    "    save_dir=exp_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f6b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all past inputs (for context)\n",
    "past_inputs = torch.cat(\n",
    "    [b for b, _ in test_loader], dim=0\n",
    ").numpy()  # (num_sequences, LOOK_BACK, num_features)\n",
    "\n",
    "# Ensure NUM_PLOTS does not exceed number of sequences\n",
    "num_sequences = y_true.shape[0]\n",
    "NUM_SUBPLOTS = min(NUM_SUBPLOTS, num_sequences)\n",
    "\n",
    "# Randomly select sequence indices for plotting\n",
    "plot_indices = np.random.choice(num_sequences, size=NUM_SUBPLOTS, replace=False)\n",
    "\n",
    "trajectory_sets = []\n",
    "\n",
    "for seq_idx in plot_indices:\n",
    "    past = past_inputs[seq_idx]  # shape: (LOOK_BACK, features)\n",
    "    true_future = y_true[seq_idx]  # shape: (seq_len, features)\n",
    "    pred_future = y_pred[seq_idx]  # shape: (seq_len, features)\n",
    "\n",
    "    # Inverse scale past and future\n",
    "    past_orig = scale_per_agent(past, scaler_X, FEATURES_PER_AGENT, inverse=True)\n",
    "    true_future_orig = scale_per_agent(\n",
    "        true_future, scaler_y, FEATURES_PER_AGENT, inverse=True\n",
    "    )\n",
    "    pred_future_orig = scale_per_agent(\n",
    "        pred_future, scaler_y, FEATURES_PER_AGENT, inverse=True\n",
    "    )\n",
    "\n",
    "    # Concatenate last past point with future to make continuous lines\n",
    "    true_line = np.vstack([past_orig[-1:], true_future_orig])\n",
    "    pred_line = np.vstack([past_orig[-1:], pred_future_orig])\n",
    "\n",
    "    trajectory_sets.append((past_orig, true_line, pred_line))\n",
    "\n",
    "# Create subplots for selected sequences\n",
    "plot_path = Path(exp_dir) / \"training_subplots.png\"\n",
    "plot_3d_trajectories_subplots(trajectory_sets, per_agent=True, save_path=str(plot_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
